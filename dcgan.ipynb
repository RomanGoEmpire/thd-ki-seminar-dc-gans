{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.api.optimizers import Adam\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.losses import BinaryCrossentropy\n",
    "from keras.api.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    LeakyReLU,\n",
    "    Reshape,\n",
    "    Conv2D,\n",
    "    Conv2DTranspose,\n",
    "    BatchNormalization,\n",
    "    LeakyReLU,\n",
    "    Flatten,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PhysicalDeviceSpec to set the device\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "def load_data() -> list:\n",
    "    (train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "    print(train_images.shape)\n",
    "    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype(\n",
    "        \"float32\"\n",
    "    )\n",
    "    train_images = (train_images - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "    return train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(epoch: int) -> None:\n",
    "    global generator\n",
    "    test_input = tf.random.normal([25, 100])\n",
    "    predictions = generator(test_input, training=False)\n",
    "\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(8, 8))\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        sns.heatmap(\n",
    "            predictions[i, :, :, 0] * 127.5 + 127.5,\n",
    "            cbar=False,\n",
    "            ax=ax,\n",
    "            xticklabels=False,\n",
    "            yticklabels=False,\n",
    "        )\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"images/image_at_epoch_{epoch:02d}.png\", bbox_inches=\"tight\", pad_inches=0\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_losses(gen_losses: list[float], disc_losses: list[float]) -> None:\n",
    "    gen_losses = np.array(gen_losses)\n",
    "    disc_losses = np.array(disc_losses)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=gen_losses, label=\"Generator Loss\", color=\"blue\")\n",
    "    sns.lineplot(data=disc_losses, label=\"Discriminator Loss\", color=\"red\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator model\n",
    "def create_generator() -> Sequential:\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Input((100,)),\n",
    "            Dense(256 * 7 * 7),\n",
    "            LeakyReLU(0.01),\n",
    "            Reshape((7, 7, 256)),\n",
    "            Conv2DTranspose(128, kernel_size=5, strides=1, padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(0.01),\n",
    "            Conv2DTranspose(64, kernel_size=5, strides=2, padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(0.01),\n",
    "            Conv2DTranspose(\n",
    "                1, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create the discriminator model\n",
    "def create_discriminator() -> Sequential:\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Input((28, 28, 1)),\n",
    "            Conv2D(32, kernel_size=3, strides=2, padding=\"same\"),\n",
    "            LeakyReLU(0.01),\n",
    "            Conv2D(64, kernel_size=3, strides=2, padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(0.01),\n",
    "            Conv2D(128, kernel_size=3, strides=2, padding=\"same\"),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(0.01),\n",
    "            Flatten(),\n",
    "            Dense(1),\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output: list, fake_output: list) -> float:\n",
    "    global cross_entropy\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def generator_loss(fake_output: list) -> list:\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train_step(images: list) -> tuple[float, float]:\n",
    "    global generator, discriminator, batch_size, generator_optimizer, discriminator_optimizer\n",
    "\n",
    "    noise = tf.random.normal([batch_size, 100])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(\n",
    "        disc_loss, discriminator.trainable_variables\n",
    "    )\n",
    "\n",
    "    generator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_generator, generator.trainable_variables)\n",
    "    )\n",
    "    discriminator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_discriminator, discriminator.trainable_variables)\n",
    "    )\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train(dataset: list, epochs: int) -> None:\n",
    "    global gen_losses, disc_losses\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for image_batch in dataset:\n",
    "            gen_loss, disc_loss = train_step(image_batch)\n",
    "            gen_losses.append(gen_loss)\n",
    "            disc_losses.append(disc_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Gen Loss: {gen_loss}, Disc Loss: {disc_loss}\")\n",
    "        generate_and_save_images(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 59_968\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "gen_losses = []\n",
    "disc_losses = []\n",
    "\n",
    "train_images = load_data()\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(train_images)\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = Adam(learning_rate=0.0002)\n",
    "discriminator_optimizer = Adam(learning_rate=0.0002)\n",
    "\n",
    "cross_entropy = BinaryCrossentropy(from_logits=True)\n",
    "generator = create_generator()\n",
    "discriminator = create_discriminator()\n",
    "\n",
    "train(train_dataset, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(gen_losses, disc_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
